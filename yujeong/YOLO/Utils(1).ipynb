{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d60c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import math\n",
    "import time\n",
    "import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def to_cpu(tensor):##\n",
    "    return tensor.detach().cpu() ##tensor에서 이루어진 연산 기록을 분리해 cpu메모리로 복사해 반환\n",
    "                                  ## cpu에 보내는 코드\n",
    "\n",
    "def load_classes(path):\n",
    "    \"\"\"\n",
    "    Loads class labels at 'path'\n",
    "    \"\"\"\n",
    "    fp = open(path, \"r\")\n",
    "    names = fp.read().split(\"\\n\")[:-1]\n",
    "    return names\n",
    "\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "def rescale_boxes(boxes, current_dim, original_shape):\n",
    "    \"\"\" Rescales bounding boxes to the original shape \"\"\"\n",
    "    orig_h, orig_w = original_shape\n",
    "    # The amount of padding that was added\n",
    "    pad_x = max(orig_h - orig_w, 0) * (current_dim / max(original_shape))\n",
    "    pad_y = max(orig_w - orig_h, 0) * (current_dim / max(original_shape))\n",
    "    # Image height and width after padding is removed\n",
    "    unpad_h = current_dim - pad_y\n",
    "    unpad_w = current_dim - pad_x\n",
    "    # Rescale bounding boxes to dimension of original image\n",
    "    boxes[:, 0] = ((boxes[:, 0] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 1] = ((boxes[:, 1] - pad_y // 2) / unpad_h) * orig_h\n",
    "    boxes[:, 2] = ((boxes[:, 2] - pad_x // 2) / unpad_w) * orig_w\n",
    "    boxes[:, 3] = ((boxes[:, 3] - pad_y // 2) / unpad_h) * orig_h\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def xywh2xyxy(x):\n",
    "    y = x.new(x.shape)\n",
    "    y[..., 0] = x[..., 0] - x[..., 2] / 2\n",
    "    y[..., 1] = x[..., 1] - x[..., 3] / 2\n",
    "    y[..., 2] = x[..., 0] + x[..., 2] / 2\n",
    "    y[..., 3] = x[..., 1] + x[..., 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def ap_per_class(tp, conf, pred_cls, target_cls):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
    "    # Arguments\n",
    "        tp:    True positives (list).\n",
    "        conf:  Objectness value from 0-1 (list).\n",
    "        pred_cls: Predicted object classes (list).\n",
    "        target_cls: True object classes (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # Sort by objectness\n",
    "    i = np.argsort(-conf)\n",
    "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
    "\n",
    "    # Find unique classes\n",
    "    unique_classes = np.unique(target_cls)\n",
    "\n",
    "    # Create Precision-Recall curve and compute AP for each class\n",
    "    ap, p, r = [], [], []\n",
    "    for c in tqdm.tqdm(unique_classes, desc=\"Computing AP\"):\n",
    "        i = pred_cls == c\n",
    "        n_gt = (target_cls == c).sum()  # Number of ground truth objects\n",
    "        n_p = i.sum()  # Number of predicted objects\n",
    "\n",
    "        if n_p == 0 and n_gt == 0:\n",
    "            continue\n",
    "        elif n_p == 0 or n_gt == 0:\n",
    "            ap.append(0)\n",
    "            r.append(0)\n",
    "            p.append(0)\n",
    "        else:\n",
    "            # Accumulate FPs and TPs\n",
    "            fpc = (1 - tp[i]).cumsum()\n",
    "            tpc = (tp[i]).cumsum()\n",
    "\n",
    "            # Recall\n",
    "            recall_curve = tpc / (n_gt + 1e-16)\n",
    "            r.append(recall_curve[-1])\n",
    "\n",
    "            # Precision\n",
    "            precision_curve = tpc / (tpc + fpc)\n",
    "            p.append(precision_curve[-1])\n",
    "\n",
    "            # AP from recall-precision curve\n",
    "            ap.append(compute_ap(recall_curve, precision_curve))\n",
    "\n",
    "    # Compute F1 score (harmonic mean of precision and recall)\n",
    "    p, r, ap = np.array(p), np.array(r), np.array(ap)\n",
    "    f1 = 2 * p * r / (p + r + 1e-16)\n",
    "\n",
    "    return p, r, ap, f1, unique_classes.astype(\"int32\")\n",
    "\n",
    "\n",
    "def compute_ap(recall, precision):\n",
    "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
    "    Code originally from https://github.com/rbgirshick/py-faster-rcnn.\n",
    "    # Arguments\n",
    "        recall:    The recall curve (list).\n",
    "        precision: The precision curve (list).\n",
    "    # Returns\n",
    "        The average precision as computed in py-faster-rcnn.\n",
    "    \"\"\"\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.0], recall, [1.0]))\n",
    "    mpre = np.concatenate(([0.0], precision, [0.0]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "        mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "    return ap\n",
    "\n",
    "\n",
    "def get_batch_statistics(outputs, targets, iou_threshold):\n",
    "    \"\"\" Compute true positives, predicted scores and predicted labels per sample \"\"\"\n",
    "    batch_metrics = []\n",
    "    for sample_i in range(len(outputs)):\n",
    "\n",
    "        if outputs[sample_i] is None:\n",
    "            continue\n",
    "\n",
    "        output = outputs[sample_i]\n",
    "        pred_boxes = output[:, :4]\n",
    "        pred_scores = output[:, 4]\n",
    "        pred_labels = output[:, -1]\n",
    "\n",
    "        true_positives = np.zeros(pred_boxes.shape[0])\n",
    "\n",
    "        annotations = targets[targets[:, 0] == sample_i][:, 1:]\n",
    "        target_labels = annotations[:, 0] if len(annotations) else []\n",
    "        if len(annotations):\n",
    "            detected_boxes = []\n",
    "            target_boxes = annotations[:, 1:]\n",
    "\n",
    "            for pred_i, (pred_box, pred_label) in enumerate(zip(pred_boxes, pred_labels)):\n",
    "\n",
    "                # If targets are found break\n",
    "                if len(detected_boxes) == len(annotations):\n",
    "                    break\n",
    "\n",
    "                # Ignore if label is not one of the target labels\n",
    "                if pred_label not in target_labels:\n",
    "                    continue\n",
    "\n",
    "                iou, box_index = bbox_iou(pred_box.unsqueeze(0), target_boxes).max(0)\n",
    "                if iou >= iou_threshold and box_index not in detected_boxes:\n",
    "                    true_positives[pred_i] = 1\n",
    "                    detected_boxes += [box_index]\n",
    "        batch_metrics.append([true_positives, pred_scores, pred_labels])\n",
    "    return batch_metrics\n",
    "\n",
    "\n",
    "def bbox_wh_iou(wh1, wh2):\n",
    "    wh2 = wh2.t()\n",
    "    w1, h1 = wh1[0], wh1[1]\n",
    "    w2, h2 = wh2[0], wh2[1]\n",
    "    inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "    union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "    return inter_area / union_area\n",
    "\n",
    "\n",
    "def bbox_iou(box1, box2, x1y1x2y2=True):\n",
    "    \"\"\"\n",
    "    Returns the IoU of two bounding boxes\n",
    "    \"\"\"\n",
    "    if not x1y1x2y2:\n",
    "        # Transform from center and width to exact coordinates\n",
    "        b1_x1, b1_x2 = box1[:, 0] - box1[:, 2] / 2, box1[:, 0] + box1[:, 2] / 2\n",
    "        b1_y1, b1_y2 = box1[:, 1] - box1[:, 3] / 2, box1[:, 1] + box1[:, 3] / 2\n",
    "        b2_x1, b2_x2 = box2[:, 0] - box2[:, 2] / 2, box2[:, 0] + box2[:, 2] / 2\n",
    "        b2_y1, b2_y2 = box2[:, 1] - box2[:, 3] / 2, box2[:, 1] + box2[:, 3] / 2\n",
    "    else:\n",
    "        # Get the coordinates of bounding boxes\n",
    "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[:, 0], box1[:, 1], box1[:, 2], box1[:, 3]\n",
    "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[:, 0], box2[:, 1], box2[:, 2], box2[:, 3]\n",
    "\n",
    "    # get the corrdinates of the intersection rectangle\n",
    "    inter_rect_x1 = torch.max(b1_x1, b2_x1)\n",
    "    inter_rect_y1 = torch.max(b1_y1, b2_y1)\n",
    "    inter_rect_x2 = torch.min(b1_x2, b2_x2)\n",
    "    inter_rect_y2 = torch.min(b1_y2, b2_y2)\n",
    "    # Intersection area\n",
    "    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(\n",
    "        inter_rect_y2 - inter_rect_y1 + 1, min=0\n",
    "    )\n",
    "    # Union Area\n",
    "    b1_area = (b1_x2 - b1_x1 + 1) * (b1_y2 - b1_y1 + 1)\n",
    "    b2_area = (b2_x2 - b2_x1 + 1) * (b2_y2 - b2_y1 + 1)\n",
    "\n",
    "    iou = inter_area / (b1_area + b2_area - inter_area + 1e-16)\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def non_max_suppression(prediction, conf_thres=0.5, nms_thres=0.4): # 가장 정확한 bbox 찾기\n",
    "    \"\"\"\n",
    "    Removes detections with lower object confidence score than 'conf_thres' and performs\n",
    "    object confidence score가 conf_thres보다 낮은 값을 가지면 제거\n",
    "    Non-Maximum Suppression to further filter detections.\n",
    "    Returns detections with shape:\n",
    "        (x1, y1, x2, y2, object_conf, class_score, class_pred)\n",
    "    \"\"\"\n",
    "    # From (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "    prediction[..., :4] = xywh2xyxy(prediction[..., :4])\n",
    "    output = [None for _ in range(len(prediction))]\n",
    "    for image_i, image_pred in enumerate(prediction):\n",
    "        # Filter out confidence scores below threshold\n",
    "        image_pred = image_pred[image_pred[:, 4] >= conf_thres]\n",
    "        # If none are remaining => process next image\n",
    "        if not image_pred.size(0):\n",
    "            continue\n",
    "        # Object confidence times class confidence\n",
    "        score = image_pred[:, 4] * image_pred[:, 5:].max(1)[0]\n",
    "        # Sort by it\n",
    "        image_pred = image_pred[(-score).argsort()]\n",
    "        class_confs, class_preds = image_pred[:, 5:].max(1, keepdim=True)\n",
    "        detections = torch.cat((image_pred[:, :5], class_confs.float(), class_preds.float()), 1)\n",
    "        # Perform non-maximum suppression\n",
    "        keep_boxes = []\n",
    "        while detections.size(0):\n",
    "            large_overlap = bbox_iou(detections[0, :4].unsqueeze(0), detections[:, :4]) > nms_thres\n",
    "            label_match = detections[0, -1] == detections[:, -1]\n",
    "            # Indices of boxes with lower confidence scores, large IOUs and matching labels\n",
    "            invalid = large_overlap & label_match\n",
    "            weights = detections[invalid, 4:5]\n",
    "            # Merge overlapping bboxes by order of confidence\n",
    "            detections[0, :4] = (weights * detections[invalid, :4]).sum(0) / weights.sum()\n",
    "            keep_boxes += [detections[0]]\n",
    "            detections = detections[~invalid]\n",
    "        if keep_boxes:\n",
    "            output[image_i] = torch.stack(keep_boxes)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "#이미지에서 box loss를 구하는 함수\n",
    "def build_targets(pred_boxes, pred_cls, target, anchors, ignore_thres): ##\n",
    "    ByteTensor = torch.cuda.ByteTensor if pred_boxes.is_cuda else torch.ByteTensor\n",
    "    FloatTensor = torch.cuda.FloatTensor if pred_boxes.is_cuda else torch.FloatTensor\n",
    "    #GPU혹은 CPU에 있는 각 텐서를 가져온다\n",
    "    \n",
    "    \n",
    "    nB = pred_boxes.size(0)\n",
    "    nA = pred_boxes.size(1)\n",
    "    nC = pred_cls.size(-1)\n",
    "    nG = pred_boxes.size(2)\n",
    "\n",
    "    # Output tensors\n",
    "    obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "    noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "    class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    iou_scores = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "    tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "\n",
    "    # Convert to position relative to box\n",
    "    target_boxes = target[:, 2:6] * nG\n",
    "    gxy = target_boxes[:, :2]\n",
    "    gwh = target_boxes[:, 2:]\n",
    "    # Get anchors with best iou\n",
    "    # iou= Intersection over Union으로 박스에서 겹쳐져 있는 영역 중 적절한 것 검출\n",
    "    ious = torch.stack([bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "    best_ious, best_n = ious.max(0)\n",
    "    # Separate target values\n",
    "    b, target_labels = target[:, :2].long().t()\n",
    "    gx, gy = gxy.t()\n",
    "    gw, gh = gwh.t()\n",
    "    gi, gj = gxy.long().t()\n",
    "    # Set masks\n",
    "    obj_mask[b, best_n, gj, gi] = 1\n",
    "    noobj_mask[b, best_n, gj, gi] = 0\n",
    "\n",
    "    # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "    for i, anchor_ious in enumerate(ious.t()):\n",
    "        noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "\n",
    "    # Coordinates\n",
    "    tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "    ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "    # Width and height\n",
    "    tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "    th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "    # One-hot encoding of label\n",
    "    tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "    # Compute label correctness and iou at best anchor\n",
    "    class_mask[b, best_n, gj, gi] = (pred_cls[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "    iou_scores[b, best_n, gj, gi] = bbox_iou(pred_boxes[b, best_n, gj, gi], target_boxes, x1y1x2y2=False)\n",
    "\n",
    "    tconf = obj_mask.float()\n",
    "    return iou_scores, class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf\n",
    "\n",
    "\n",
    "def parse_model_config(path):  ##\n",
    "    \"\"\"Parses the yolo-v3 layer configuration file and returns module definitions\"\"\"\n",
    "    #욜로레이어구성을 분석하고 모듈정의를 반환함 config 파일 분석\n",
    "    file = open(path, 'r')\n",
    "    lines = file.read().split('\\n')\n",
    "    lines = [x for x in lines if x and not x.startswith('#')]#파일에서 주석을 제외한 코드 읽어들임\n",
    "    lines = [x.rstrip().lstrip() for x in lines]  # get rid of fringe whitespaces 공백제거\n",
    "    module_defs = []\n",
    "    for line in lines:\n",
    "        if line.startswith('['):  # This marks the start of a new block\n",
    "            module_defs.append({})\n",
    "            module_defs[-1]['type'] = line[1:-1].rstrip()\n",
    "            if module_defs[-1]['type'] == 'convolutional':\n",
    "                module_defs[-1]['batch_normalize'] = 0\n",
    "        else:\n",
    "            key, value = line.split(\"=\")\n",
    "            value = value.strip()\n",
    "            module_defs[-1][key.rstrip()] = value.strip()\n",
    "\n",
    "    return module_defs #분석한 파일\n",
    "\n",
    "\n",
    "def parse_data_config(path):\n",
    "    \"\"\"Parses the data configuration file\"\"\"\n",
    "    options = dict()\n",
    "    options['gpus'] = '0,1,2,3'\n",
    "    options['num_workers'] = '10'\n",
    "    with open(path, 'r') as fp:\n",
    "        lines = fp.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line == '' or line.startswith('#'):\n",
    "            continue\n",
    "        key, value = line.split('=')\n",
    "        options[key.strip()] = value.strip()\n",
    "    return options\n",
    "\n",
    "\n",
    "def ResizePadding(height, width):\n",
    "    desized_size = (height, width)\n",
    "\n",
    "    def resizePadding(image, **kwargs): # 입력 이미지를 설정된 크기로 변경\n",
    "        old_size = image.shape[:2] # image의 (height, width)값\n",
    "        max_size_idx = old_size.index(max(old_size)) # height와 width중 큰 값의 index 저장\n",
    "        ratio = float(desized_size[max_size_idx]) / max(old_size) # 384 / old_size의 max\n",
    "        new_size = tuple([int(x * ratio) for x in old_size]) \n",
    "\n",
    "        if new_size > desized_size:\n",
    "            min_size_idx = old_size.index(min(old_size))\n",
    "            ratio = float(desized_size[min_size_idx]) / min(old_size)\n",
    "            new_size = tuple([int(x * ratio) for x in old_size])\n",
    "\n",
    "        image = cv2.resize(image, (new_size[1], new_size[0]))\n",
    "        delta_w = desized_size[1] - new_size[1]\n",
    "        delta_h = desized_size[0] - new_size[0]\n",
    "        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
    "        left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
    "\n",
    "        image = cv2.copyMakeBorder(image, top, bottom, left, right, cv2.BORDER_CONSTANT)\n",
    "        return image\n",
    "    return resizePadding\n",
    "\n",
    "\n",
    "class AverageValueMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        self.val = 0\n",
    "\n",
    "    def add(self, value, n=1):\n",
    "        self.val = value\n",
    "        self.sum += value\n",
    "        self.var += value * value\n",
    "        self.n += n\n",
    "\n",
    "        if self.n == 0:\n",
    "            self.mean, self.std = np.nan, np.nan\n",
    "        elif self.n == 1:\n",
    "            self.mean = 0.0 + self.sum  # This is to force a copy in torch/numpy\n",
    "            self.std = np.inf\n",
    "            self.mean_old = self.mean\n",
    "            self.m_s = 0.0\n",
    "        else:\n",
    "            self.mean = self.mean_old + (value - n * self.mean_old) / float(self.n)\n",
    "            self.m_s += (value - self.mean_old) * (value - self.mean)\n",
    "            self.mean_old = self.mean\n",
    "            self.std = np.sqrt(self.m_s / (self.n - 1.0))\n",
    "\n",
    "    def value(self):\n",
    "        return self.mean, self.std\n",
    "\n",
    "    def reset(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0.0\n",
    "        self.var = 0.0\n",
    "        self.val = 0.0\n",
    "        self.mean = np.nan\n",
    "        self.mean_old = 0.0\n",
    "        self.m_s = 0.0\n",
    "        self.std = np.nan"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
